<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html xmlns:v="urn:schemas-microsoft-com:vml"
 xmlns:o="urn:schemas-microsoft-com:office:office"
 xmlns="http://www.w3.org/TR/REC-html40">
<head>
  <meta name="GENERATOR" content="Microsoft FrontPage 5.0">
  <meta name="ProgId" content="FrontPage.Editor.Document">
  <meta http-equiv="Content-Type"
 content="text/html; charset=windows-1252">
  <title>Parts and Attributes</title>
  <meta name="Microsoft Theme" content="none, default">
</head>
<body bgproperties="fixed" style="background-color: rgb(255, 255, 255);">
<p style="text-align: left; font-family: Arial;">
<big><big><font><font><font><font><font><font><font><span
 style="font-weight: bold;"><img style="width: 632px; height: 155px;"
 alt="" src="pna.JPG"></span></font></font></font></font></font></font></font></big></big></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><big><big><font><font><font><font><font><font><font><span
 style="font-weight: bold;"></span></font></font></font></font></font></font></font></big></big><br>
<big><big><font><font><font><font><font><font><font><span
 style="font-weight: bold;">Parts and Attributes</span></font></font></font></font></font></font></font></big></big></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Third
International Workshop on Parts and Attributes</p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">In
Conjunction with the European Conference on Computer Vision (<a
 href="http://eccv2014.org/">ECCV
2014</a>)</p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Date:
September 12th, 2014</p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Venue:
Zurich, Switzerland</p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Photos</p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><a
 href="https://picasaweb.google.com/116328339939918154883/PnA2014?authuser=0&amp;authkey=Gv1sRgCM6ytoe74KaYNA&amp;feat=embedwebsite"><img
 style="border: 0px solid ; width: 900px; height: 517px;" alt=""
 src="CollagePnA2014.jpg"></a></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Overview</p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><span
 style="font-weight: normal;">The workshop will bring
together
researchers from the established field of part-based methods and from
the field of attribute-based methods, which has recently gained
popularity. Participants will learn from each other about recent
developments and applications, for example in object recognition, scene
classification and image retrieval, and they will have the opportunity
to discuss similarities and differences, advantages and disadvantages
of both approaches.&nbsp;</span></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><span
 style="font-weight: normal;">&nbsp; </span></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Organizers
</p>
<table style="text-align: center; width: 1000px;" border="0"
 cellpadding="2" cellspacing="2">
  <tbody>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><img
 style="width: 101px; height: 131px;" alt="" src="rogerio.jpg"><br>
      </td>
      <td><img style="width: 131px; height: 131px;" alt=""
 src="christoph.gif"><br>
      </td>
      <td><img style="width: 130px; height: 130px;" alt=""
 src="devi.jpg"><br>
      </td>
    </tr>
    <tr>
      <td>
      <p style="text-align: center; font-family: Arial;"><a
 href="http://rogerioferis.com/">Rogerio Feris</a></p>
      </td>
      <td>
      <p style="text-align: center; font-family: Arial;"><a
 href="http://pub.ist.ac.at/%7Echl/">Christoph Lampert</a></p>
      </td>
      <td>
      <p style="text-align: center; font-family: Arial;"><a
 href="https://deviparikh.com">Devi
Parikh</a></p>
      </td>
    </tr>
  </tbody>
</table>
<p style="text-align: left; font-weight: bold; font-family: Arial;">&nbsp;
</p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Schedule</p>
<table style="font-family: Arial;" border="2" cellpadding="7"
 cellspacing="7" width="92%">
  <tbody>
    <tr>
      <td bordercolor="#000000" width="57">08:40</td>
      <td bordercolor="#000000" align="center" width="113">&nbsp;</td>
      <td bordercolor="#000000">Welcome<br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">08:45</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/Thomas.jpg" border="0" height="110" width="100"></td>
      <td bordercolor="#000000">Invited
Talk: <b> <a href="https://staff.fnwi.uva.nl/t.e.j.mensink/index.php">Thomas
Mensink</a></b>
(University of Amsterdam) - <span
 style="font-weight: bold; font-style: italic;">"COSTA:
Co-Occurrence
Statistics for Zero-Shot Classification"</span><br>
[<a href="slides/mensink.pdf">Slides</a>]<br>
      <br>
In this talk I will introduce the first zero-shot classification method
for multi-labeled image datasets. Our method, COSTA, exploits
co-occurrences of visual concepts in images for knowledge transfer.
These inter-dependencies arise naturally between concepts in
multi-labelled datasets, and are easy to obtain from existing
annotations or web-search hit counts. We estimate a classifier for a
new label, as a weighted combination of related classes, using the
co-occurrences to define the weight. We also propose a regression model
for learning a weight for each label in the training set, which we
learn in a leave-one-out setting, which improves significantly the
performance. Finally, we also show that our zero-shot classifiers can
serve as priors for few-shot learning. Experiments on three
multi-labeled datasets reveal that our proposed zero-shot methods, are
approaching and occasionally outperforming fully supervised SVMs. We
conclude that co-occurrence statistics suffice for zero-shot
classification.</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">09:20</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/ali.jpg" border="0" height="128" width="100"></td>
      <td bordercolor="#000000">Invited
Talk: <b> <a href="http://homes.cs.washington.edu/%7Eali/">Ali
Farhadi</a></b>
(University of Washington)&nbsp;- <i><b>"Attributes at Scale"</b></i></td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">09:55</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/raquel.jpg" border="0" height="117" width="100"></td>
      <td bordercolor="#000000">Invited
Talk: <b> <a href="http://www.cs.toronto.edu/%7Eurtasun/">Raquel
Urtasun</a></b>
(University of Toronto) - <i><b>"Understanding
Complex Scenes and
People that Talk about Them"</b></i><br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">10:30</td>
      <td bordercolor="#000000" align="center" width="113">&nbsp;</td>
      <td bordercolor="#000000">Coffee
Break<br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">11:00</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/gregory.jpg" border="0" height="117" width="100"></td>
      <td bordercolor="#000000">Invited
Talk: <b> <a href="http://psych.nyu.edu/murphy/">Gregory Murphy</a></b>
(New York
University) - <i><b>"When
Are Categories More Useful Than Attributes?
A Perspective From Induction"<br>
      </b></i>[<a href="./slides/murphy.ppt">Slides</a>]<i><b><br>
      </b></i>&nbsp;<br>
Categories are useful because they allow us to infer attributes of an
object that were not themselves observed during categorization.
However, some attributes can be directly inferred from an object's
perceptible properties. I discuss two sets of experiments that test
whether people make such attribute-to-attribute inductions, and whether
they rely more on these or on category-to-attribute inductions.</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">11:35</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/adriana.jpg" border="0" height="135" width="100"></td>
      <td bordercolor="#000000">Invited
Talk: <b> <a href="http://www.cs.utexas.edu/%7Eadriana/">Adriana
Kovashka</a></b>
(University of Pittsburgh) - <i><b>"Interactive
Image Search with
Attributes"</b></i><br>
[<a href="slides/kovashka.pptx">Slides</a>]<br>
      <br>
Search engines have come a long way, but searching for images is still
primarily restricted to meta information such as keywords as opposed to
the images' visual content. We introduce a new form of interaction for
image retrieval, where the user can give rich feedback to the system
via semantic visual attributes. The proposed WhittleSearch approach
allows users to narrow down the pool of relevant images by comparing
the properties of the results to those of the desired target. Building
on this idea, we develop a system-guided version of the method which
actively engages the user in a 20-questions-like game where the answers
are visual comparisons. This enables the system to obtain that
information which it most needs to know. To ensure that the system
interprets the user's attribute-based queries and feedback as intended,
we further show how to efficiently adapt a generic model for an
attribute to more closely align with the individual user's perception.
Our work transforms the interaction between the image search system and
its user from keywords and clicks to precise and natural language-based
communication. We demonstrate the dramatic impact of this new search
modality for effective retrieval on databases ranging from consumer
products to human faces. This is an important step in making the output
of vision systems more useful, by allowing users to both express their
needs better and better interpret the system's predictions. <br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">12:10</td>
      <td bordercolor="#000000" align="center" width="113">&nbsp;</td>
      <td bordercolor="#000000">Lunch
Break<br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">14:00</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/niloy.jpg" border="0" height="126" width="100"></td>
      <td bordercolor="#000000">Invited
Talk: <b> <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/">Niloy
Mitra</a></b>
(University College London) - <i><b>"Abstracting
Collections of
Objects
and Scenes"</b></i><br>
[<a href="slides/mitra.pdf">Slides</a>]<br>
      <br>
3D data continues to grow in the form of collections of models, scenes,
scans, or of course as image collections. Such data, when appropriately
abstracted and represented, can provide valuable priors for many
geometry processing tasks, including editing, synthesis, and
form-finding. In this talk, I will discuss the various algorithms we
have developed over the last years to co-analyze large 3D data
collections and represent them as probability distributions over
part-based abstractions. Such an approach focuses more on the global
inter and intra semantic relations among the parts of the shape rather
than on their local geometric details. Beyond analysis techniques, I
will discuss applications in editing, modeling, and fabrication.<br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">14:35</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/peter.jpg" border="0" height="108" width="100"></td>
      <td bordercolor="#000000">Invited
Talk: <b> <a href="http://files.is.tue.mpg.de/pgehler/">Peter
Gehler</a></b>
(Max
Planck Institute) - <i><b>"Fields
of Parts"</b></i><br>
[<a href="slides/gehler.pdf">Slides</a>]<br>
      <br>
Part based models are ubiquitous in human pose estimation and object
detection. In this talk I will present the Fields of Parts model that
offers a different viewpoint on the classical Pictorial Structures
model. The Fields of Parts model can be understood as an unrolled mean
field inference machine. We train it with a maximum margin estimator
using mean-field backpropagation. I will establish the link between the
PS model, the Fields of Parts model, and a multilayer neural network
with convolutional kernels. I will argue that it offers interesting new
modeling flexibility as it paves the way to joint body pose estimation
and segmentation.<br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">15:10</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/poster.jpg" border="0" height="102" width="100"></td>
      <td bordercolor="#000000"><b>Poster
Session</b> (and Coffee Break)<br>
      <br>
Check the<b> <a
 href="https://deviparikh.com/projects/PnA2014/posters/Posters.html">
list of accepted posters</a></b><br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">16:40</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/shih-fu.jpg" border="0" height="132" width="100"></td>
      <td bordercolor="#000000">Invited
Talk: <b> <a href="http://www.ee.columbia.edu/%7Esfchang/">Shih-Fu
Chang</a></b>
(Columbia University) - <i><b>"Concept-Based
Framework for Detecting
High-Level Events in Video"</b></i><br>
      <br>
Attributes and parts are intuitive representations for real-world
objects and have been shown effective in recent research on object
recognition. An analogous framework has been used in the multimedia
community using "concepts" for describing high-level complex events
such as "birthday party" or "changing a vehicle tire." Concepts involve
objects, scenes, actions, activities, and other syntactic elements
usually seen in video events. In this talk, I will address several
fundamental issues encountered when developing concept-based event
framework - how to determine the basic concepts needed by humans when
annotating video events; how to use Web mining to automatically
discover a large concept pool for event representation; how to handle
the weak supervision problem when concept labels are assigned to long
video clips without precise timing; and finally how the concept
classifier pool can be used to help retrieve novel events that have not
been seen before (namely the zero-shot retrieval problem).<br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">17:15</td>
      <td bordercolor="#000000" align="center" width="113"> <img
 src="schedule/pics/serge.jpg" border="0" height="111" width="100"></td>
      <td bordercolor="#000000">Invited
Talk: <b> <a href="http://cseweb.ucsd.edu/%7Esjb/">Serge Belongie</a></b>
(Cornell
Tech) - <i><b>"Visipedia
Tool Ecosystem"</b></i><br>
[<a href="slides/belongie.pdf">Slides</a>]<br>
      <br>
To support scalable computer vision applications, we have built a suite
of tools that allow for efficient collection and annotation of large
image datasets. The tools are designed to both reduce data management
overhead and foster collaborations between vision researchers and
groups seeking the benefits of a computer vision application.<br>
&nbsp;</td>
    </tr>
    <tr>
      <td bordercolor="#000000" width="57">17:50</td>
      <td bordercolor="#000000" align="center" width="113">&nbsp;</td>
      <td bordercolor="#000000">Concluding
Remarks</td>
    </tr>
  </tbody>
</table>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Important
Dates</p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><span
 style="font-weight: normal;">Submission deadline: <span
 style="text-decoration: line-through;">June 30th</span>
July 25th, 2014, 11:59 pm EST</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Notification
of acceptance: <span style="text-decoration: line-through;">July
10th</span>, July 31st, 2014</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Camera
ready submission: July 17th,
2014</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Workshop
date: September 12th, 2014</span></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Submissions<br>
<span style="font-weight: normal;"></span></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><span
 style="font-weight: normal;">Four-page (excluding
references) extended
abstracts in ECCV 2014 format. Abstracts describing new, published
(e.g. at main ECCV 2014 conference) or&nbsp;ongoing work are
welcome. There will be no proceedings.</span></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><span
 style="font-weight: normal;">The extended abstract
should be submitted
as a single PDF file via email to the workshop organizers.</span><span
 style="font-weight: normal;"></span></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><span
 style="font-weight: normal;">Contributions from the
following domains
or closely-related areas are especially welcome:</span><br
 style="font-weight: normal;">
<br style="font-weight: normal;">
<span style="font-weight: normal;">Deformable
and rigid part-based
models</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Generative
and discriminative
part-based models</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Unsupervised
discovery of parts</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Context
and hierarchy in part-based
models</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Part-sharing
methods for visual
recognition</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Learning
visual attributes across
object classes</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Attribute-based
classification and
search</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Semantic
attributes as object
representations</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Mid-level
representations based on
parts/attributes</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Transfer
learning / Zero-shot
learning</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Fine-grained
visual categorization
based on parts and attributes</span><br style="font-weight: normal;">
<span style="font-weight: normal;">Innovative
applications related to
parts and attributes</span><br>
<br>
<span style="font-weight: normal;">Accepted
submissions will be
presented as posters at the workshop.&nbsp;&nbsp;</span></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><span
 style="font-weight: normal;">Reviewing of abstract
submissions will be
single-blind i.e. submissions need not be anonymized.</span><br>
</p>
<p style="text-align: left; font-weight: bold; font-family: Arial;">Previous
Iterations</p>
<p style="text-align: left; font-family: Arial;"><a
 href="http://pub.ist.ac.at/%7Echl/PnA2012/">In conjunction with ECCV
2012</a></p>
<p style="text-align: left; font-weight: bold; font-family: Arial;"><a
 href="http://rogerioferis.com/PartsAndAttributes/"><span
 style="font-weight: normal;">In conjunction with ECCV 2010</span></a></p>
<!-- Start of StatCounter Code -->
<script type="text/javascript">
var sc_project=1299599; var sc_invisible=1; var sc_partition=63; var sc_click_stat=1; var sc_security="3d8147d8"; </script>
<script type="text/javascript"
 src="http://www.statcounter.com/counter/counter.js"></script>
<noscript><div
class="statcounter"><a title="free web stats"
href="http://www.statcounter.com/" target="_blank"><img
class="statcounter"
src="http://c.statcounter.com/1299599/0/3d8147d8/1/"
alt="free web stats" ></a></div></noscript>
<!-- End of StatCounter Code -->
</body>
</html>
