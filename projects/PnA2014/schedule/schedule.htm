<html>

<head>
<meta http-equiv="Content-Language" content="en-us">
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>Schedule</title>
</head>

<body topmargin="30" leftmargin="30">

<table border="2" width="92%" cellspacing="7" cellpadding="7">
	<tr>
		<td width="57" bordercolor="#000000">08:40</td>
		<td width="113" bordercolor="#000000" align="center">&nbsp;</td>
		<td bordercolor="#000000">Welcome<br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">08:45</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/Thomas.jpg" width="100" height="110"></td>
		<td bordercolor="#000000">Invited Talk: <b>
		<a href="https://staff.fnwi.uva.nl/t.e.j.mensink/index.php">Thomas 
		Mensink</a></b> (University of Amsterdam)</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">09:20</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/ali.jpg" width="100" height="128"></td>
		<td bordercolor="#000000">Invited Talk: <b>
		<a href="http://homes.cs.washington.edu/~ali/">Ali Farhadi</a></b> (University of 
		Washington)</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">09:55</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/raquel.jpg" width="100" height="117"></td>
		<td bordercolor="#000000">Invited Talk: <b>
		<a href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a></b> (University of 
		Toronto) - <i><b>&quot;Understanding Complex Scenes and People that Talk about 
		Them&quot;</b></i><br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">10:30</td>
		<td width="113" bordercolor="#000000" align="center">&nbsp;</td>
		<td bordercolor="#000000">Coffee Break<br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">11:00</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/gregory.jpg" width="100" height="117"></td>
		<td bordercolor="#000000">Invited Talk: <b>
		<a href="http://psych.nyu.edu/murphy/">Gregory Murphy</a></b> (New York 
		University) - <i><b>&quot;When Are Categories More Useful Than Attributes? A 
		Perspective From Induction&quot;</b></i><br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">11:35</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/adriana.jpg" width="100" height="135"></td>
		<td bordercolor="#000000">Invited Talk: <b>
		<a href="http://www.cs.utexas.edu/~adriana/">Adriana Kovashka</a></b> (University of 
		Pittsburgh) - <i><b>&quot;Interactive Image Search with Attributes&quot;</b></i><br>
		<br>
		Search engines have come a long way, but searching for images is still 
		primarily restricted to meta information such as keywords as opposed to 
		the images' visual content. We introduce a new form of interaction for 
		image retrieval, where the user can give rich feedback to the system via 
		semantic visual attributes. The proposed WhittleSearch approach allows 
		users to narrow down the pool of relevant images by comparing the 
		properties of the results to those of the desired target. Building on 
		this idea, we develop a system-guided version of the method which 
		actively engages the user in a 20-questions-like game where the answers 
		are visual comparisons. This enables the system to obtain that 
		information which it most needs to know. To ensure that the system 
		interprets the user's attribute-based queries and feedback as intended, 
		we further show how to efficiently adapt a generic model for an 
		attribute to more closely align with the individual user's perception. 
		Our work transforms the interaction between the image search system and 
		its user from keywords and clicks to precise and natural language-based 
		communication. We demonstrate the dramatic impact of this new search 
		modality for effective retrieval on databases ranging from consumer 
		products to human faces. This is an important step in making the output 
		of vision systems more useful, by allowing users to both express their 
		needs better and better interpret the system's predictions. <br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">12:10</td>
		<td width="113" bordercolor="#000000" align="center">&nbsp;</td>
		<td bordercolor="#000000">Lunch Break<br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">14:00</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/niloy.jpg" width="100" height="126"></td>
		<td bordercolor="#000000">Invited Talk: <b>
		<a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/">Niloy Mitra</a></b> (University College 
		London)- <i><b>&quot;Abstracting Collections of Objects and Scenes&quot;</b></i><br>
		<br>
		3D data continues to grow in the form of collections of models, scenes, 
		scans, or of course as image collections. Such data, when appropriately 
		abstracted and represented, can provide valuable priors for many 
		geometry processing tasks, including editing, synthesis, and 
		form-finding. In this talk, I will discuss the various algorithms we 
		have developed over the last years to co-analyze large 3D data 
		collections and represent them as probability distributions over 
		part-based abstractions. Such an approach focuses more on the global 
		inter and intra semantic relations among the parts of the shape rather 
		than on their local geometric details. Beyond analysis techniques, I 
		will discuss applications in editing, modeling, and fabrication.<br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">14:35</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/peter.jpg" width="100" height="108"></td>
		<td bordercolor="#000000">Invited Talk: <b>
		<a href="http://files.is.tue.mpg.de/pgehler/">Peter Gehler</a></b> (Max Planck 
		Institute) - <i><b>&quot;Fields of Parts&quot;</b></i><br>
		<br>
		Part based models are ubiquitous in human pose estimation and object 
		detection. In this talk I will present the Fields of Parts model that 
		offers a different viewpoint on the classical Pictorial Structures 
		model. The Fields of Parts model can be understood as an unrolled mean 
		field inference machine. We train it with a maximum margin estimator 
		using mean-field backpropagation. I will establish the link between the 
		PS model, the Fields of Parts model, and a multilayer neural network 
		with convolutional kernels. I will argue that it offers interesting new 
		modeling flexibility as it paves the way to joint body pose estimation 
		and segmentation.<br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">15:10</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/poster.jpg" width="100" height="102"></td>
		<td bordercolor="#000000"><b>Poster Session</b> (and Coffee Break)<br>
		<br>
		Check the<b>
		<a href="https://filebox.ece.vt.edu/~parikh/PnA2014/posters/Posters.html">
		list of accepted posters</a></b><br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">16:40</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/shih-fu.jpg" width="100" height="132"></td>
		<td bordercolor="#000000">Invited Talk: <b>
		<a href="http://www.ee.columbia.edu/~sfchang/">Shih-Fu Chang</a></b> (Columbia 
		University) - <i><b>&quot;Concept-Based Framework for Detecting High-Level Events 
		in Video&quot;</b></i><br>
		<br>
		Attributes and parts are intuitive representations for real-world 
		objects and have been shown effective in recent research on object 
		recognition. An analogous framework has been used in the multimedia 
		community using &quot;concepts&quot; for describing high-level complex events such 
		as &quot;birthday party&quot; or &quot;changing a vehicle tire.&quot; Concepts involve 
		objects, scenes, actions, activities, and other syntactic elements 
		usually seen in video events. In this talk, I will address several 
		fundamental issues encountered when developing concept-based event 
		framework - how to determine the basic concepts needed by humans when 
		annotating video events; how to use Web mining to automatically discover 
		a large concept pool for event representation; how to handle the weak 
		supervision problem when concept labels are assigned to long video clips 
		without precise timing; and finally how the concept classifier pool can 
		be used to help retrieve novel events that have not been seen before 
		(namely the zero-shot retrieval problem).<br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">17:15</td>
		<td width="113" bordercolor="#000000" align="center">
		<img border="0" src="pics/serge.jpg" width="100" height="111"></td>
		<td bordercolor="#000000">Invited Talk: <b>
		<a href="http://cseweb.ucsd.edu/~sjb/">Serge Belongie</a></b> (Cornell Tech) - 
		<i><b>&quot;Visipedia 
		Tool Ecosystem&quot;</b></i><br>
		<br>
		To support scalable computer vision applications, we have built a suite 
		of tools that allow for efficient collection and annotation of large 
		image datasets. The tools are designed to both reduce data management 
		overhead and foster collaborations between vision researchers and groups 
		seeking the benefits of a computer vision application.<br>
&nbsp;</td>
	</tr>
	<tr>
		<td width="57" bordercolor="#000000">17:50</td>
		<td width="113" bordercolor="#000000" align="center">&nbsp;</td>
		<td bordercolor="#000000">Concluding Remarks</td>
	</tr>
	</table>

</body>

</html>